{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 images\n",
      "satImage_001.png\n",
      "Loading 20 images\n",
      "satImage_001.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400, 400, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)\n",
    "        gt_img_3c[:, :, 0] = gt_img8\n",
    "        gt_img_3c[:, :, 1] = gt_img8\n",
    "        gt_img_3c[:, :, 2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0, imgheight, h):\n",
    "        for j in range(0, imgwidth, w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j : j + w, i : i + h]\n",
    "            else:\n",
    "                im_patch = im[j : j + w, i : i + h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(20, len(files))  # Load maximum 20 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 16, 16, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size = 16\n",
    "\n",
    "img_patches = [img_crop(imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "gt_patches = [img_crop(gt_imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "\n",
    "# Linearize list of patches\n",
    "img_patches = np.asarray(\n",
    "    [\n",
    "        img_patches[i][j]\n",
    "        for i in range(len(img_patches))\n",
    "        for j in range(len(img_patches[i]))\n",
    "    ]\n",
    ")\n",
    "gt_patches = np.asarray(\n",
    "    [\n",
    "        gt_patches[i][j]\n",
    "        for i in range(len(gt_patches))\n",
    "        for j in range(len(gt_patches[i]))\n",
    "    ]\n",
    ")\n",
    "\n",
    "img_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a simple FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        # Input shape: (3, 16, 16)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        )\n",
    "\n",
    "        self.unconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 11, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.unconv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FCN                                      [1, 16, 16]               --\n",
       "├─Sequential: 1-1                        [32, 8, 8]                --\n",
       "│    └─Conv2d: 2-1                       [16, 16, 16]              448\n",
       "│    └─ReLU: 2-2                         [16, 16, 16]              --\n",
       "│    └─MaxPool2d: 2-3                    [16, 8, 8]                --\n",
       "│    └─Conv2d: 2-4                       [32, 8, 8]                12,832\n",
       "├─Sequential: 1-2                        [1, 16, 16]               --\n",
       "│    └─ConvTranspose2d: 2-5              [16, 8, 8]                12,816\n",
       "│    └─ReLU: 2-6                         [16, 8, 8]                --\n",
       "│    └─ConvTranspose2d: 2-7              [1, 16, 16]               1,937\n",
       "│    └─Sigmoid: 2-8                      [1, 16, 16]               --\n",
       "==========================================================================================\n",
       "Total params: 28,033\n",
       "Trainable params: 28,033\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 5.07\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.06\n",
       "Params size (MB): 0.11\n",
       "Estimated Total Size (MB): 0.17\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(FCN(), input_size=(3, 16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_img_patches = np.asarray(img_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, device, train_loader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        output = output > 0.5\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(loss.item())\n",
    "        acc_history.append(output.eq(target).sum() / target.numel())\n",
    "\n",
    "    return acc_history, loss_history\n",
    "\n",
    "def get_mean_std(imgs):\n",
    "    \"\"\"\n",
    "    Normalizes images with mean and standard deviation, by channel\n",
    "    \"\"\"\n",
    "    mean = imgs.mean(axis=(0, 2, 3))\n",
    "    std = imgs.std(axis=(0, 2, 3))\n",
    "    return mean, std\n",
    "\n",
    "def train(device):\n",
    "    # creating the dataloader\n",
    "    images = n_img_patches\n",
    "    groundtruth = Tensor(gt_patches)\n",
    "\n",
    "    mean, std = get_mean_std(images)\n",
    "    print(mean, std)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[mean[0], mean[1], mean[2]], std=[std[0], std[1], std[2]])\n",
    "    ])\n",
    "    images = torch.stack(([transform(img) for img in images]))\n",
    "    \n",
    "    pytorchDl = DataLoader(TensorDataset(images,groundtruth),batch_size = 32,shuffle=True)\n",
    "    model = FCN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.functional.binary_cross_entropy\n",
    "\n",
    "    for i in range(10):\n",
    "        acc_history, loss_history = train_epoch(model, device, pytorchDl, optimizer, i, criterion)\n",
    "        print(\"Epoch: \", i, \" Accuracy: \", sum(acc_history)/len(acc_history), \" Loss: \", sum(loss_history)/len(loss_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30062687 0.29847103 0.2976581  0.29828402 0.29836816 0.29749525\n",
      " 0.2979489  0.30031368 0.3005086  0.2996019  0.29961902 0.30030555\n",
      " 0.3008969  0.30131173 0.30097467 0.30262402] [0.17090596 0.17101276 0.17084403 0.1707926  0.17001918 0.1703739\n",
      " 0.16964005 0.16982245 0.1687028  0.16795427 0.16863145 0.16967691\n",
      " 0.17056271 0.171323   0.17065597 0.1712765 ]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marwa\\Desktop\\EPFL\\ml\\ml-project-2\\fcn.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/marwa/Desktop/EPFL/ml/ml-project-2/fcn.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;32mc:\\Users\\marwa\\Desktop\\EPFL\\ml\\ml-project-2\\fcn.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marwa/Desktop/EPFL/ml/ml-project-2/fcn.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marwa/Desktop/EPFL/ml/ml-project-2/fcn.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marwa/Desktop/EPFL/ml/ml-project-2/fcn.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[mean[\u001b[39m0\u001b[39m], mean[\u001b[39m1\u001b[39m], mean[\u001b[39m2\u001b[39m]], std\u001b[39m=\u001b[39m[std[\u001b[39m0\u001b[39m], std[\u001b[39m1\u001b[39m], std[\u001b[39m2\u001b[39m]])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marwa/Desktop/EPFL/ml/ml-project-2/fcn.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marwa/Desktop/EPFL/ml/ml-project-2/fcn.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([transform(img) \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marwa/Desktop/EPFL/ml/ml-project-2/fcn.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m pytorchDl \u001b[39m=\u001b[39m DataLoader(TensorDataset(images,groundtruth),batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m,shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marwa/Desktop/EPFL/ml/ml-project-2/fcn.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m model \u001b[39m=\u001b[39m FCN()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marwa/Desktop/EPFL/ml/ml-project-2/fcn.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marwa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "\u001b[1;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "train(torch.device(\"cuda\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
